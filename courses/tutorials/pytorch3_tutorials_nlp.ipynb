{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will walk you through the key ideas of deep learning programming using Pytorch. Many of the concepts (such as the computation graph abstraction and autograd) are not unique to Pytorch and are relevant to any deep learning toolkit out there.\n",
    "\n",
    "I am writing this tutorial to focus specifically on NLP for people who have never written code in any deep learning framework (e.g, TensorFlow, Theano, Keras, Dynet). It assumes working knowledge of core NLP problems: part-of-speech tagging, language modeling, etc. It also assumes familiarity with neural networks at the level of an intro AI class (such as one from the Russel and Norvig book). Usually, these courses cover the basic backpropagation algorithm on feed-forward neural networks, and make the point that they are chains of compositions of linearities and non-linearities. This tutorial aims to get you started writing deep learning code, given you have this prerequisite knowledge.\n",
    "\n",
    "Note this is about models, not data. For all of the models, I just create a few test examples with small dimensionality so you can see how the weights change as it trains. If you have some real data you want to try, you should be able to rip out any of the models from this notebook and use them on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to Torch’s tensor library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的深度学习都是在张量上计算的,其中张量是一个可以被超过二维索引的矩阵的一般化. 稍后我们将详细讨论这意味着什么.首先,我们先来看一下我们可以用张量来干什么."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f585797870>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量可以在Python list形式下通过torch.Tensor()函数创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.Tensor(V_data)\n",
    "\n",
    "print(V)\n",
    "\n",
    "print(V.size())\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.Tensor(M_data)\n",
    "\n",
    "print(M)\n",
    "\n",
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.Tensor(T_data)\n",
    "\n",
    "print(T)\n",
    "\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是三维张量? 让我们这样想象.如果你有一个向量,那么对向量索引就会得到一个标量. 如果你有一个矩阵,对矩阵索引那么就会得到一个向量.如果你有一个三维张量,那么对其索引 就会得到一个矩阵!\n",
    "\n",
    "针对术语的说明: 当我在本教程内使用”tensor”,它针对的是所有torch.Tensor对象.矩阵和向量是特殊的torch.Tensors, 他们的维度分别是1和2.当我说到三维张量,我会简洁的使用”3D tensor”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以创建其他数据类型的tensors.默认的数据类型为Float(浮点型). 可以使用torch.LongTensor()来 创建一个整数类型的tensor.你可以在文件中寻找更多的数据类型,但是Float(浮点型)和Long(长整形)最常用的.\n",
    "\n",
    "你可以使用torch.randn()创建一个随机数据和需要提供维度的tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.6614  0.2669  0.0617  0.6213 -0.4519\n",
      " -0.1661 -1.5228  0.3817 -1.0276 -0.5631\n",
      " -0.8923 -0.0583 -0.1955 -0.9656  0.4224\n",
      "  0.2673 -0.4212 -0.5107 -1.5727 -0.1232\n",
      "\n",
      "(1 ,.,.) = \n",
      "  3.5870 -1.8313  1.5987 -1.2770  0.3255\n",
      " -0.4791  1.3790  2.5286  0.4107 -0.9880\n",
      " -0.9081  0.5423  0.1103 -2.2590  0.6067\n",
      " -0.1383  0.8310 -0.2477 -0.8029  0.2366\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.2857  0.6898 -0.6331  0.8795 -0.6842\n",
      "  0.4533  0.2912 -0.8317 -0.5525  0.6355\n",
      " -0.3968 -0.6571 -1.6428  0.9803 -0.0421\n",
      " -0.8206  0.3133 -1.1352  0.3773 -0.2824\n",
      "[torch.FloatTensor of size 3x4x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以以你想要的方式操作tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "y = torch.Tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以查阅 文档 获取大量可用操作的完整列表, 扩展到了非数学操作. http://pytorch.org/docs/torch.html\n",
    "\n",
    "接下来一个很有帮助的操作就是连接."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.5667 -1.4303  0.5009  0.5438 -0.4057\n",
      " 1.1341 -1.1115  0.3501 -0.7703 -0.1473\n",
      " 0.6272  1.0935  0.0939  1.2381 -1.3459\n",
      " 0.5119 -0.6933 -0.1668 -0.9999 -1.6476\n",
      " 0.8098  0.0554  1.1340 -0.5326  0.6592\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      "-1.5964 -0.3769 -3.1020 -0.0020 -1.0952  0.6016  0.6984 -0.8005\n",
      "-0.0995 -0.7213  1.2708  1.5381  1.4673  1.5951 -1.5279  1.0156\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.view()去重构tensor.这是一个高频方法, 因为许多神经网络的神经元对输入格式 有明确的要求. 你通常需要先将数据重构再输入到神经元中."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -0.2020 -1.2865  0.8231 -0.6101\n",
      " -1.2960 -0.9434  0.6684  1.1628\n",
      " -0.3229  1.8782 -0.5666  0.4016\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.1153  0.3170  0.5629  0.8662\n",
      " -0.3528  0.3482  1.1371 -0.3339\n",
      " -1.4724  0.7296 -0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2020 -1.2865  0.8231 -0.6101 -1.2960 -0.9434  0.6684  1.1628 -0.3229  1.8782\n",
      "-0.1153  0.3170  0.5629  0.8662 -0.3528  0.3482  1.1371 -0.3339 -1.4724  0.7296\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.5666  0.4016\n",
      "-0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2020 -1.2865  0.8231 -0.6101 -1.2960 -0.9434  0.6684  1.1628 -0.3229  1.8782\n",
      "-0.1153  0.3170  0.5629  0.8662 -0.3528  0.3482  1.1371 -0.3339 -1.4724  0.7296\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.5666  0.4016\n",
      "-0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Computation Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图的思想对于有效率的深度学习编程是很重要的, 因为它允许你不必去自己写反向梯度传播. 计算图只是简单地说明了如何将数据组合在一起以输出结果.因为图完全指定了操作所包含的参数, 因此它包含了足够的信息去求导.这可能听起来很模糊, 所以让我们看看使用Pytorch的基本类: autograd.Variable.\n",
    "\n",
    "首先, 从程序员的角度来思考.在torch中存储了什么, 是我们在上面创建的Tensor对象吗? 显然, 是数据和 结构, 也很可能是其他的东西. 但是当我们将两个tensors相加后, 我们得到了一个输出tensor.这个输出所能 体现出的只有数据和结构, 并不能体现出是由两个tensors加和得到的(因为它可能是从一个文件中读取的, 也可能是 其他操作的结果等).\n",
    "\n",
    "变量类别可以一直跟踪它是如何创建的.让我们在实际中来看."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = autograd.Variable(torch.Tensor([1., 2., 3.]), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With requires_grad=True, you can still do all the operations you previously could\n",
    "y = autograd.Variable(torch.Tensor([4., 5., 6.]), requires_grad=True)\n",
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x000001F587E390F0>\n"
     ]
    }
   ],
   "source": [
    "# BUT z knows something extra.\n",
    "# if x and y all set requires_grad=True, this function returns None\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<AccumulateGrad object at 0x000001F587E392B0>, 0), (<AccumulateGrad object at 0x000001F587E396D8>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(z.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然变量知道怎么创建的它们. z知道它并非是从文件读取的, 也不是乘法或指数或其他运算的结果. 如果你继续跟踪 z.grad_fn, 你会从中找到x和y的痕迹.\n",
    "\n",
    "但是它如何帮助我们计算梯度?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<SumBackward0 object at 0x000001F587E39400>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么这个计算和对x的第一个分量的导数等于多少? 在数学上,我们求 ds/dx, s知道它是被tensor z的和创建的.z 知道它是x+y的和, 并且s包含了足够的信息去决定我们需要的导数为1!\n",
    "\n",
    "当然它掩盖了如何计算导数的挑战.这是因为s携带了足够多的信息所以导数可以被计算.现实中,Pytorch 程序的开发人员用程序指令sum()和 + 操作以知道如何计算它们的梯度并且运行反向传播算法.深入讨论此算法 超出了本教程的范围.\n",
    "\n",
    "让我们用Pytorch计算梯度,发现我们是对的:(如果你运行这个方块很多次,梯度会上升,这是因为Pytorch accumulates (累积) 渐变为.grad属性, 因为对于很多模型它是很方便的.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个成功的深度学习程序员了解下面的方块如何运行是至关重要的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x000001F587E48208>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 2))\n",
    "y = torch.randn((2, 2))\n",
    "z = x + y  # 这些是Tensor类型,反向是不可能的\n",
    "\n",
    "var_x = autograd.Variable(x, requires_grad=True)\n",
    "var_y = autograd.Variable(y, requires_grad=True)\n",
    "# var_z 包含了足够的信息去计算梯度,如下所示\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)\n",
    "\n",
    "var_z_data = var_z.data  # 从 var_z中得到包裹Tensor对象...\n",
    "# 在一个新的变量中重新包裹tensor\n",
    "new_var_z = autograd.Variable(var_z_data)\n",
    "\n",
    "# new_var_z 有去反向x和y的信息吗?\n",
    "# 没有!\n",
    "print(new_var_z.grad_fn)\n",
    "# 怎么会这样? 我们将 tensor 从 var_z 中提取 (提取为var_z.data). 这个张量不知道它是如\n",
    "# 何计算的.我们把它传递给 new_var_z.\n",
    "# 这就是new_var_z得到的所有信息. 如果 var_z_data 不知道它是如何计算的, 那么就不会有 new_var_z 的方法.\n",
    "# 从本质上讲, 我们已经把这个变量从过去的历史中分离出来了.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是基础的,但是对于计算自动求导是特别重要的规则 (这比Pytorch更通用,在每个主要的深度学习工具箱中都有一个相同的对象):\n",
    "\n",
    "**如果你想要从损失函数返回到神经网络的某个神经元得到错误,那么你就不能将断开从该组件到你的丢失变量的变量链.如果你这样做, 损失将不知道你的组件存在, 并且它的参数不能被更新.**\n",
    "\n",
    "我用粗体表示, 因为这个错误会在不经意间发生(我将在下面展示一些这样的方法), 并且它不会导致您的代码崩溃或报错, 所以您必须小心."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Deep Learning Building Blocks: Affine maps, non-linearities and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习以巧妙的方式将non-linearities和linearities组合在一起.non-linearities的引入允许强大的模型. 在本节中, 我们将使用这些核心组件, 构建一个objective函数, 并且看看模型是如何训练的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的核心工作之一是affine map, 这是一个函数f(x) 其中\n",
    "\n",
    "f(x) = Ax + b\n",
    "\n",
    "对于矩阵 A 和向量 x,b. 这里学习的参数是 A and b. 通常, b 被称为 偏差 项.\n",
    "\n",
    "Pytorch和大多数其他深度学习框架与传统的线性代数有所不同.它映射输入的是行而不是列. 也就是说, 下面的输出的第 i 行是 A 的输入的第 i 行加上偏置项的映射. 看下面的例子."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f585797870>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.1115  0.3501 -0.7703 -0.1473  0.6272\n",
      " 1.0935  0.0939  1.2381 -1.3459  0.5119\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "Variable containing:\n",
      "-0.0236 -0.3005  0.2450\n",
      " 0.3692 -1.3302  0.0736\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5,3)\n",
    "data = autograd.Variable(torch.randn(2, 5))\n",
    "print(data)\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先, 注意以下将解释为什么我们首先需要 non-linearities.假设我们有两个 affine maps f(x)=Ax+b and g(x)=Cx+d. 什么是 f(g(x))?\n",
    "- f(g(x))=A(Cx+d)+b=ACx+(Ad+b)\n",
    "\n",
    "AC 是一个矩阵, Ad+b 是一个向量, 所以我们看到组合两个affine maps会得到一个affine map\n",
    "\n",
    "由此可以看出, 如果你想让你的神经网络成为affine 组合的长链条, 那么相比于做一个简单的affine map, 此举不会给你的模型增加新的作用.\n",
    "\n",
    "如果我们在affine层之间引入non-linearities, 则不再是这种情况, 我们可以构建更强大的模型.\n",
    "\n",
    "接下来有一些重要的non-linearities. tanh(x),σ(x),ReLU(x) 是最常见的. 你可能想知道: “为什么这些函数？我可以想到很多其他的non-linearities 函数.” 其原因是他们的梯度容易计算, 并且计算梯度对学习是必不可少的. 例如\n",
    "- dσ/dx=σ(x)(1−σ(x))\n",
    "\n",
    "一个简单的提示: 虽然你可能已经在入门AI中学习到了一些神经网络, 其中 σ(x) 是默认的non-linearity, 但通常人们在实践中会避免它. 这是因为随着参数绝对值的增长, 梯度会很快 消失 . 小梯度意味着很难学习. 大多数人默认tanh或ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.8098  0.0554\n",
      " 1.1340 -0.5326\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.8098  0.0554\n",
      " 1.1340  0.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = autograd.Variable(torch.randn(2, 2))\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 Softmax(x) 也是一个 non-linearity, 但它的特殊之处在于它通常是网络中一次操作. 这是因为它接受了一个实数向量并返回一个概率分布.其定义如下. 定义 x 是一个实数的向量(正数或负数都无所谓, 没有限制). 然后, 第i个 Softmax(x) 的组成是\n",
    "- exp(xi)/∑jexp(xj)\n",
    "\n",
    "应该清楚的是, 输出是一个概率分布: 每个元素都是非负的, 并且所有元素的总和都是1.\n",
    "\n",
    "你也可以把它看作只是将一个元素明确的指数运算符应用于输入, 以使所有内容都为非负值, 然后除以归一化常数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6592\n",
      "-1.5964\n",
      "-0.3769\n",
      "-3.1020\n",
      "-0.0995\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 0.5125\n",
      " 0.0537\n",
      " 0.1819\n",
      " 0.0119\n",
      " 0.2400\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.6684\n",
      "-2.9240\n",
      "-1.7045\n",
      "-4.4297\n",
      "-1.4271\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = autograd.Variable(torch.randn(5))\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_softmax\n",
    "- While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function 是一个目标函数，你训练网络的目的是使其最小(在这种情况下, 它通常被称为 loss function 或 cost function ). 首先选择一个训练实例, 通过神经网络运行它, 计算输出的损失. 然后利用损失函数的导数来更新模型的参数. 直观地说, 如果你的模型对答案完全有信心, 但答案是错误的, 你的损失就会很高. 如果它的答案非常有信心, 而且答案是正确的, 那么损失就会很低.\n",
    "\n",
    "将训练样例的损失函数最小化的想法是, 你的网络希望能够很好地产生, 并且在开发集, 测试集或生产环境中未知的示例有小的损失. 一个示例损失函数是 negative log likelihood loss , 这是多类分类的一个非常普遍的目标函数. 对于有监督的多类别分类, 这意味着训练网络以最小化正确输出的负对数概率(或等同地, 最大化正确输出的对数概率)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Optimization and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么我们可以计算一个实例的损失函数?我们该怎么做?我们之前看到 autograd.Variable 知道如何计算与计算梯度有关的事物.那么, 因为我们的loss是一个 autograd.Variable, 我们可以对所有用于计算的参数计算梯度！然后我们可以执行standard gradient updates. 令 θ 是我们的参数, L(θ) 损失函数, 以及: η 是一个正的的学习率. 然后:\n",
    "- θ(t+1)=θ(t)−η∇θL(θ)\n",
    "\n",
    "有大量的算法和积极的研究去尝试比vanilla gradient updates更新更出色的方法. 许多人试图根据训练的情况改变学习率. 除非你真的感兴趣, 否则你不必担心这些算法具体做什么. Torch提供了许多 torch.optim 包, 它们都是开源的.使用最简单的梯度更新与更复杂的算法效果相同.尝试不同的更新算法和更新算法的不同参数(如不同的初始学习速率)对于优化网络性能非常重要. 通常, 只需用Adam或RMSProp等优化器替换vanilla SGD 即可显着提升性能."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Creating Network Components in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们开始关注NLP之前, 让我们做一个注释的例子, 在Pytorch中只使用affine maps和non-linearities构建网络.我们还将看到如何使用Pytorch建立的negative log likelihood 计算损失函数, 并通过反向传播更新参数.\n",
    "\n",
    "所有神经元都应该从nn.Module继承并覆盖forward()方法.就样板而言就是这样.从nn.Module继承能为你的神经元提供功能.例如, 它可以跟踪其可训练的参数, 可以使用.cuda()或.cpu()函数等在CPU和GPU之间交换, 等等.\n",
    "\n",
    "我们来编写一个带有注释的网络示例, 该网络采用稀疏的词袋表示法, 并输出概率分布在两个标签上: “英语”和“西班牙语”.使用的模型是逻辑回归."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Logistic Regression Bag-of-Words classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的模型将映射一个稀疏的BOW表示来记录标签上的概率.我们为词汇表中的每个单词分配一个索引. 例如, 我们的完整的词汇表有两个单词: “hello” 和 “world”, 这两个单词的索引分别为0和1. 句子为 “hello hello hello hello” 的BoW向量为\n",
    "- [4,0]\n",
    "\n",
    "对于 “hello world world hello” , 它是\n",
    "- [2,2]\n",
    "\n",
    "etc. 一般来说, 它是\n",
    "- [Count(hello),Count(world)]\n",
    "\n",
    "将这个BOW向量表示为 x. 我们的网络输出是:\n",
    "- logSoftmax(Ax+b)\n",
    "\n",
    "也就是说, 我们通过affine map传递输入, 然后进行softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Yo', 'creo', 'que', 'si'], 'SPANISH'),\n",
       " (['it', 'is', 'lost', 'on', 'me'], 'ENGLISH')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
      "\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data+test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "print()\n",
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1) # Reshape to the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0490 -0.0159 -0.1690 -0.0387 -0.1265  0.1802 -0.1695 -0.1529 -0.0067 -0.1060\n",
      "-0.0153 -0.0063  0.0333  0.0924  0.0315  0.0598 -0.1764  0.1429  0.1710  0.1621\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0702 -0.0755 -0.0921  0.0111  0.1420 -0.1380  0.0921  0.1260  0.1918 -0.1373\n",
      " 0.1450 -0.1415 -0.0727  0.1729 -0.1494  0.1779 -0.1542 -0.1381  0.0959 -0.1409\n",
      "\n",
      "Columns 20 to 25 \n",
      " 0.0475 -0.1450  0.1674 -0.0761  0.1181  0.0058\n",
      "-0.0449  0.1427  0.1553  0.1855 -0.0398 -0.1524\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1931\n",
      "-0.0418\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 12 \n",
       "    1     1     1     1     1     1     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 13 to 25 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 1x26]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector, and it's needed to wrapped in autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix); bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.7869 -0.6074\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上哪个值对应于”英语”的概率, 以及哪个值是”西班牙语”?我们从来没有定义过它, 但如果我们想要训练这个模型, 我们需要去定义."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来做训练吧！要做到这一点, 我们通过实例来获取概率, 计算损失函数, 计算损失函数的梯度, 然后用梯度步骤更新参数.Torch在nn软件包中提供了损失函数.nn.NLLLoss()是我们想要的负对数似然损失.它还定义了torch.optim中的优化函数.在这里, 我们只使用SGD.\n",
    "\n",
    "请注意, NLLLoss 的 输入 是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率. 这就是为什么我们网络的最后一层是log softmax. 损失函数 nn.CrossEntropyLoss() 与 NLLLoss() 相同, 唯一的不同是它为你去做 softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.6338 -0.7563\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.6821 -0.7044\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001F587E2BD58>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generator object, so we can directly call next() w/o iter()\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0702\n",
      " 0.1450\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Give', 'it', 'to', 'me']\n",
      "ENGLISH\n"
     ]
    }
   ],
   "source": [
    "instance, label = data[1]\n",
    "print(instance)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    1     0     0     0     0     0     1     1     1     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x26]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "target = autograd.Variable(make_target(label, label_to_ix))\n",
    "print()\n",
    "print(bow_vec)\n",
    "print(target) # Notice this is one number, not a vector with #class dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Variable as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = criterion(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1113 -2.2507\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-2.4659 -0.0888\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5102\n",
      "-0.2951\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了正确的答案! 你可以看到, 第一个示例中西班牙语的概率要高得多, 而测试数据的第二个英语概率应该高得多.\n",
    "\n",
    "现在你看到了如何制作一个Pytorch组件, 通过它传递一些数据并做梯度更新.我们准备深入挖掘NLP所能提供的内容."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Word Embeddings: Encoding Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are dense vectors of real numbers, one per word in your vocabulary. In NLP, it is almost always the case that your features are words! But how should you represent a word in a computer? You could store its ascii character representation, but that only tells you what the word is, it doesn’t say much about what it means (you might be able to derive its part of speech from its affixes, or properties from its capitalization, but not much). Even more, in what sense could you combine these representations? We often want dense outputs from our neural networks, where the inputs are |V| dimensional, where V is our vocabulary, but often the outputs are only a few dimensional (if we are only predicting a handful of labels, for instance). How do we get from a massive dimensional space to a smaller dimensional space?\n",
    "\n",
    "How about instead of ascii representations, we use a one-hot encoding? That is, we represent the word w by\n",
    "- [0,0,…,1,…,0,0], |V| elements\n",
    "\n",
    "where the 1 is in a location unique to w. Any other word will have a 1 in some other location, and a 0 everywhere else.\n",
    "\n",
    "There is an enormous drawback to this representation, besides just how huge it is. It basically treats all words as independent entities with no relation to each other. What we really want is some notion of similarity between words. Why? Let’s see an example.\n",
    "\n",
    "Suppose we are building a language model. Suppose we have seen the sentences\n",
    "- The mathematician ran to the store.\n",
    "- The physicist ran to the store.\n",
    "- The mathematician solved the open problem.\n",
    "\n",
    "in our training data. Now suppose we get a new sentence never before seen in our training data:\n",
    "- The physicist solved the open problem.\n",
    "\n",
    "Our language model might do OK on this sentence, but wouldn’t it be much better if we could use the following two facts:\n",
    "- We have seen mathematician and physicist in the same role in a sentence. Somehow they have a semantic relation.\n",
    "- We have seen mathematician in the same role in this new unseen sentence as we are now seeing physicist.\n",
    "\n",
    "and then infer that physicist is actually a good fit in the new unseen sentence? This is what we mean by a notion of similarity: we mean semantic similarity, not simply having similar orthographic representations. It is a technique to combat the sparsity of linguistic data, by connecting the dots between what we have seen and what we haven’t. This example of course relies on a fundamental linguistic assumption: that words appearing in similar contexts are related to each other semantically. This is called the **distributional hypothesis** https://en.wikipedia.org/wiki/Distributional_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Getting Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we solve this problem? That is, how could we actually encode semantic similarity in words? Maybe we think up some semantic attributes. For example, we see that both mathematicians and physicists can run, so maybe we give these words a high score for the “is able to run” semantic attribute. Think of some other attributes, and imagine what you might score some common words on those attributes.\n",
    "\n",
    "If each attribute is a dimension, then we might give each word a vector, like this:\n",
    "- q_mathematician=[2.3 (can run),9.4 (likes coffee),−5.5 (majored in Physics),…]\n",
    "- q_physicist=[2.5 (can run),9.1 (likes coffee),6.4 (majored in Physics),…]\n",
    "\n",
    "Then we can get a measure of similarity between these words by doing:\n",
    "- Similarity(physicist,mathematician)=q_physicist⋅q_mathematician\n",
    "\n",
    "Although it is more common to normalize by the lengths:\n",
    "- Similarity(physicist,mathematician)=(q_physicist⋅q_mathematician)/(||q_physicist|| ||q_mathematician||)=cos(ϕ)\n",
    "\n",
    "Where ϕ is the angle between the two vectors. That way, extremely similar words (words whose embeddings point in the same direction) will have similarity 1. Extremely dissimilar words should have similarity -1.\n",
    "\n",
    "You can think of the sparse one-hot vectors from the beginning of this section as a special case of these new vectors we have defined, where each word basically has similarity 0, and we gave each word some unique semantic attribute. These new vectors are dense, which is to say their entries are (typically) non-zero.\n",
    "\n",
    "But these new vectors are a big pain: you could think of thousands of different semantic attributes that might be relevant to determining similarity, and how on earth would you set the values of the different attributes? Central to the idea of deep learning is that the neural network learns representations of the features, rather than requiring the programmer to design them herself. So why not just let the word embeddings be parameters in our model, and then be updated during training? This is exactly what we will do. We will have some latent semantic attributes that the network can, in principle, learn. Note that the word embeddings will probably not be interpretable. That is, although with our hand-crafted vectors above we can see that mathematicians and physicists are similar in that they both like coffee, if we allow a neural network to learn the embeddings and see that both mathematicians and physicists have a large value in the second dimension, it is not clear what that means. They are similar in some latent semantic dimension, but this probably has no interpretation to us.\n",
    "\n",
    "In summary, **word embeddings are a representation of the *semantics* of a word, efficiently encoding semantic information that might be relevant to the task at hand.** You can embed other things too: part of speech tags, parse trees, anything! The idea of feature embeddings is central to the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Word Embeddings in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to a worked example and an exercise, a few quick notes about how to use embeddings in Pytorch and in deep learning programming in general. Similar to how we defined a unique index for each word when making one-hot vectors, we also need to define an index for each word when using embeddings. These will be keys into a lookup table. That is, embeddings are stored as a **|V|×D matrix**, where D is the dimensionality of the embeddings, such that the word assigned index i has its embedding stored in the i‘th row of the matrix. In all of my code, the mapping from words to indices is a dictionary named word_to_ix.\n",
    "\n",
    "The module that allows you to use embeddings is **torch.nn.Embedding**, which takes two arguments: the vocabulary size, and the dimensionality of the embeddings.\n",
    "\n",
    "To index into this table, you must use **torch.LongTensor** (since the indices are integers, not floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f585797870>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 1\n",
      "[torch.LongTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-2.5667 -1.4303  0.5009  0.5438 -0.4057\n",
      " 1.1341 -1.1115  0.3501 -0.7703 -0.1473\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"], word_to_ix[\"world\"]])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor)) # wrap by Variable input, the output always need .view(1, -1) as next layer input\n",
    "print(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 An Example: N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in an n-gram language model, given a sequence of words w, we want to compute\n",
    "- P(wi|wi−1,wi−2,…,wi−n+1)\n",
    "\n",
    "Where wi is the ith word of the sequence.\n",
    "\n",
    "In this example, we will compute the loss function on some training examples and update the parameters with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence)-2)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'praise': 0, 'winters': 1, 'held:': 2, 'Were': 3, 'days;': 4, 'How': 5, 'within': 6, 'thriftless': 7, 'new': 8, 'say,': 9, 'see': 10, 'were': 11, 'thou': 12, \"totter'd\": 13, 'asked,': 14, 'own': 15, 'now,': 16, 'worth': 17, 'field,': 18, 'and': 19, 'child': 20, 'blood': 21, 'more': 22, 'lies,': 23, 'answer': 24, 'of': 25, 'thine': 26, 'all-eating': 27, 'fair': 28, 'sum': 29, 'treasure': 30, \"excuse,'\": 31, 'all': 32, 'dig': 33, 'by': 34, 'besiege': 35, 'sunken': 36, 'on': 37, 'be': 38, 'much': 39, 'when': 40, 'And': 41, 'mine': 42, 'thy': 43, 'small': 44, 'it': 45, 'brow,': 46, 'thine!': 47, 'count,': 48, 'old': 49, 'livery': 50, 'Thy': 51, 'forty': 52, 'eyes,': 53, 'To': 54, 'trenches': 55, 'so': 56, 'If': 57, 'in': 58, \"feel'st\": 59, 'shall': 60, 'Shall': 61, 'make': 62, 'gazed': 63, 'succession': 64, 'the': 65, 'use,': 66, 'an': 67, 'Will': 68, 'shame,': 69, 'couldst': 70, 'his': 71, \"beauty's\": 72, 'art': 73, 'where': 74, \"youth's\": 75, 'a': 76, 'made': 77, 'warm': 78, 'This': 79, 'lusty': 80, 'to': 81, 'praise.': 82, 'proud': 83, \"deserv'd\": 84, 'deep': 85, 'Then': 86, \"'This\": 87, 'old,': 88, 'Proving': 89, 'weed': 90, 'When': 91, 'being': 92, 'cold.': 93, 'beauty': 94, 'my': 95, 'Where': 96}\n"
     ]
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word:i for i,word in enumerate(vocab)}\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(1, -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1) #(batch, V)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "criterion = nn.NLLLoss()\n",
    "model = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLanguageModel(\n",
       "  (embeddings): Embedding(97, 10)\n",
       "  (linear1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[519.310714006424, 516.8290419578552, 514.3636934757233, 511.9132571220398, 509.47750878334045, 507.0546727180481, 504.6437327861786, 502.2436821460724, 499.853636264801, 497.4737927913666]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0.0\n",
    "    for context, target in trigrams:\n",
    "        \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in Variable)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "        \n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 3. Run the forward pass, getting log probabilities over next words\n",
    "        log_probs = model(context_var)\n",
    "        \n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a Variable)\n",
    "        loss = criterion(log_probs, autograd.Variable(torch.LongTensor([word_to_ix[target]])))\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data[0]\n",
    "    losses.append(total_loss)\n",
    "\n",
    "# The loss decreased every iteration over the training data!\n",
    "print(losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Exercise: Computing Word Embeddings: Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word wi and an N context window on each side, wi−1,…,wi−N and wi+1,…,wi+N, referring to all context words collectively as C, CBOW tries to minimize\n",
    "- −logp(wi|C)=−logSoftmax(A(∑w∈C qw)+b)\n",
    "\n",
    "where qw is the embedding for word w.\n",
    "\n",
    "Implement this model in Pytorch by filling in the class below. Some tips:\n",
    "- Think about which parameters you need to define.\n",
    "- Make sure you know what shape each operation expects. Use .view() if you need to reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Sequence Models and Long-Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have seen various feed-forward networks. That is, there is no state maintained by the network at all. This might not be the behavior we want. Sequence models are central to NLP: they are models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field.\n",
    "\n",
    "A recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propogate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state ht, which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 LSTM’s in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting to the example, note a few things. Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. **Input of shape (seq_len, batch, input_size)**\n",
    "- The first axis is the sequence itself, \n",
    "- the second indexes instances in the mini-batch, \n",
    "- and the third indexes elements of the input.\n",
    "\n",
    "The input can also be a packed **variable length** sequence. See **torch.nn.utils.rnn.pack_padded_sequence()** or **torch.nn.utils.rnn.pack_sequence()** for details.\n",
    "\n",
    "We haven’t discussed mini-batching, so lets just ignore that and assume we will always have just 1 dimension on the second axis. If we want to run the sequence model over the sentence “The cow jumped”, our input should look like\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
    "   q_\\text{cow} \\\\\n",
    "   q_\\text{jumped}\n",
    "   \\end{bmatrix}\\end{align}\n",
    "\n",
    "\n",
    "Except remember there is an additional 2nd dimension with size 1.\n",
    "\n",
    "In addition, you could go through the sequence one at a time, in which case the 1st axis will have size 1 also.\n",
    "\n",
    "Let’s see a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f585797870>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      " 0.4533  0.2912 -0.8317\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-0.5525  0.6355 -0.3968\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-0.6571 -1.6428  0.9803\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-0.0421 -0.8206  0.3133\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-1.1352  0.3773 -0.2824\n",
      "[torch.FloatTensor of size 1x3]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# make a sequence of length 5\n",
    "inputs = [autograd.Variable(torch.randn((1,3))) for _ in range(5)]\n",
    "print(inputs) # 5*1*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -2.5667 -1.4303  0.5009\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.5438 -0.4057  1.1341\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the hidden state.\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), # 1*1*3 for h\n",
    "          autograd.Variable(torch.randn(1, 1, 3))) # 1*1*3 for c\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.5158 -0.0452  0.5860\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.5158 -0.0452  0.5860\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  1.2370 -0.0902  0.9707\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n",
      "=======\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.2656 -0.2105  0.3044\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.2656 -0.2105  0.3044\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.7071 -0.2950  0.5684\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n",
      "=======\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1058 -0.0410  0.4638\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1058 -0.0410  0.4638\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1456 -0.1012  0.8353\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n",
      "=======\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1966 -0.0293  0.4417\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1966 -0.0293  0.4417\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3199 -0.0558  0.7408\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n",
      "=======\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0533 -0.0727  0.3635\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0533 -0.0727  0.3635\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0960 -0.1245  0.6142\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden) # fit the hidden into lstm to track the [hidden state, cell state]\n",
    "    print(out)\n",
    "    print(hidden)\n",
    "    print('=======')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.LSTM(*args, **kwargs)\n",
    "- Parameters:\t\n",
    " - input_size – The number of expected features in the input x\n",
    " - hidden_size – The number of features in the hidden state h\n",
    " - num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    " - bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    " - batch_first – If True, then the input and output tensors are provided as (batch, seq, feature)\n",
    " - dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    " - bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "- Inputs: input, (h_0, c_0)\n",
    " - input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    " - h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch.\n",
    " - c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.\n",
    " - If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.\n",
    "\n",
    "- Outputs: output, (h_n, c_n)\n",
    " - output of shape (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    " - h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    " - c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "\n",
    "# Add the extra 2nd dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.4533  0.2912 -0.8317\n",
      "-0.5525  0.6355 -0.3968\n",
      "-0.6571 -1.6428  0.9803\n",
      "-0.0421 -0.8206  0.3133\n",
      "-1.1352  0.3773 -0.2824\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(inputs) # default dim=0\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.4533  0.2912 -0.8317\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.5525  0.6355 -0.3968\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.6571 -1.6428  0.9803\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.0421 -0.8206  0.3133\n",
      "\n",
      "(4 ,.,.) = \n",
      " -1.1352  0.3773 -0.2824\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.view(5, 1, -1)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.3170  0.5629  0.8662\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3528  0.3482  1.1371\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn(1, 1, 3)))\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.2440 -0.0258  0.3401\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0774 -0.1039  0.2000\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.3938 -0.0046  0.4113\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.4511  0.0169  0.4237\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.1882 -0.0359  0.3567\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1882 -0.0359  0.3567\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3341 -0.0658  0.5682\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "out, hidden = lstm(inputs, hidden) #input: 5*1*3\n",
    "print(out) # out: 5*1*3\n",
    "print(hidden) # hidden: 2*(1*1*3), one for h and one for c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Example: An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use an LSTM to get part of speech tags. We will\n",
    "not use Viterbi or Forward-Backward or anything like that, but as a\n",
    "(challenging) exercise to the reader, think about how Viterbi could be\n",
    "used after you have seen what is going on.\n",
    "\n",
    "The model is as follows: let our input sentence be\n",
    "$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let\n",
    "$T$ be our tag set, and $y_i$ the tag of word $w_i$.\n",
    "Denote our prediction of the tag of word $w_i$ by\n",
    "$\\hat{y}_i$.\n",
    "\n",
    "This is a structure prediction, model, where our output is a sequence\n",
    "$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
    "state at timestep $i$ as $h_i$. Also, assign each tag a\n",
    "unique index (like how we had word\\_to\\_ix in the word embeddings\n",
    "section). Then our prediction rule for $\\hat{y}_i$ is\n",
    "\n",
    "\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n",
    "\n",
    "That is, take the log softmax of the affine map of the hidden state,\n",
    "and the predicted tag is the tag that has the maximum value in this\n",
    "vector. Note this implies immediately that the dimensionality of the\n",
    "target space of $A$ is $|T|$.\n",
    "\n",
    "\n",
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']), (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.2976 -0.8606 -1.1910\n",
      "-1.3122 -0.8940 -1.1339\n",
      "-1.2810 -0.8799 -1.1795\n",
      "-1.2906 -0.8681 -1.1870\n",
      "-1.3309 -0.8544 -1.1704\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0322 -3.5142 -6.2351\n",
      "-4.9764 -0.0133 -5.0654\n",
      "-4.4516 -3.6952 -0.0372\n",
      "-0.0138 -4.7282 -5.3295\n",
      "-4.7061 -0.0194 -4.5925\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "# for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print (tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.3 Exercise: Augmenting the LSTM part-of-speech tagger with character-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, each word had an embedding, which served as the\n",
    "inputs to our sequence model. Let's augment the word embeddings with a\n",
    "representation derived from the characters of the word. We expect that\n",
    "this should help significantly, since character-level information like\n",
    "affixes have a large bearing on part-of-speech. For example, words with\n",
    "the affix *-ly* are almost always tagged as adverbs in English.\n",
    "\n",
    "To do this, let $c_w$ be the character-level representation of\n",
    "word $w$. Let $x_w$ be the word embedding as before. Then\n",
    "the input to our sequence model is the concatenation of $x_w$ and\n",
    "$c_w$. So if $x_w$ has dimension 5, and $c_w$\n",
    "dimension 3, then our LSTM should accept an input of dimension 8.\n",
    "\n",
    "To get the character level representation, do an LSTM over the\n",
    "characters of a word, and let $c_w$ be the final hidden state of\n",
    "this LSTM. Hints:\n",
    "\n",
    "* There are going to be two LSTM's in your new model.\n",
    "  The original one that outputs POS tag scores, and the new one that\n",
    "  outputs a character-level representation of each word.\n",
    "* To do a sequence model over characters, you will have to embed characters.\n",
    "  The character embeddings will be the input to the character LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Advanced: Making Dynamic Decisions and the Bi-LSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Dynamic versus Static Deep Learning Toolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a *dynamic* neural network kit. Another example of a dynamic\n",
    "kit is `Dynet <https://github.com/clab/dynet>`__ (I mention this because\n",
    "working with Pytorch and Dynet is similar. If you see an example in\n",
    "Dynet, it will probably help you implement it in Pytorch). The opposite\n",
    "is the *static* tool kit, which includes Theano, Keras, TensorFlow, etc.\n",
    "The core difference is the following:\n",
    "\n",
    "* In a static toolkit, you define\n",
    "  a computation graph once, compile it, and then stream instances to it.\n",
    "* In a dynamic toolkit, you define a computation graph *for each\n",
    "  instance*. It is never compiled and is executed on-the-fly\n",
    "\n",
    "Without a lot of experience, it is difficult to appreciate the\n",
    "difference. One example is to suppose we want to build a deep\n",
    "constituent parser. Suppose our model involves roughly the following\n",
    "steps:\n",
    "\n",
    "* We build the tree bottom up\n",
    "* Tag the root nodes (the words of the sentence)\n",
    "* From there, use a neural network and the embeddings\n",
    "  of the words to find combinations that form constituents. Whenever you\n",
    "  form a new constituent, use some sort of technique to get an embedding\n",
    "  of the constituent. In this case, our network architecture will depend\n",
    "  completely on the input sentence. In the sentence \"The green cat\n",
    "  scratched the wall\", at some point in the model, we will want to combine\n",
    "  the span $(i,j,r) = (1, 3, \\text{NP})$ (that is, an NP constituent\n",
    "  spans word 1 to word 3, in this case \"The green cat\").\n",
    "\n",
    "However, another sentence might be \"Somewhere, the big fat cat scratched\n",
    "the wall\". In this sentence, we will want to form the constituent\n",
    "$(2, 4, NP)$ at some point. The constituents we will want to form\n",
    "will depend on the instance. If we just compile the computation graph\n",
    "once, as in a static toolkit, it will be exceptionally difficult or\n",
    "impossible to program this logic. In a dynamic toolkit though, there\n",
    "isn't just 1 pre-defined computation graph. There can be a new\n",
    "computation graph for each instance, so this problem goes away.\n",
    "\n",
    "Dynamic toolkits also have the advantage of being easier to debug and\n",
    "the code more closely resembling the host language (by that I mean that\n",
    "Pytorch and Dynet look more like actual Python code than Keras or\n",
    "Theano)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Bi-LSTM Conditional Random Field Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this section, we will see a full, complicated example of a Bi-LSTM\n",
    "Conditional Random Field for named-entity recognition. The LSTM tagger\n",
    "above is typically sufficient for part-of-speech tagging, but a sequence\n",
    "model like the CRF is really essential for strong performance on NER.\n",
    "Familiarity with CRF's is assumed. Although this name sounds scary, all\n",
    "the model is is a CRF but where an LSTM provides the features. This is\n",
    "an advanced model though, far more complicated than any earlier model in\n",
    "this tutorial. If you want to skip it, that is fine. To see if you're\n",
    "ready, see if you can:\n",
    "\n",
    "-  Write the recurrence for the viterbi variable at step i for tag k.\n",
    "-  Modify the above recurrence to compute the forward variables instead.\n",
    "-  Modify again the above recurrence to compute the forward variables in\n",
    "   log-space (hint: log-sum-exp)\n",
    "\n",
    "If you can do those three things, you should be able to understand the\n",
    "code below. Recall that the CRF computes a conditional probability. Let\n",
    "$y$ be a tag sequence and $x$ an input sequence of words.\n",
    "Then we compute\n",
    "\n",
    "\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}\n",
    "\n",
    "Where the score is determined by defining some log potentials\n",
    "$\\log \\psi_i(x,y)$ such that\n",
    "\n",
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}\n",
    "\n",
    "To make the partition function tractable, the potentials must look only\n",
    "at local features.\n",
    "\n",
    "In the Bi-LSTM CRF, we define two kinds of potentials: emission and\n",
    "transition. The emission potential for the word at index $i$ comes\n",
    "from the hidden state of the Bi-LSTM at timestep $i$. The\n",
    "transition scores are stored in a $|T|x|T|$ matrix\n",
    "$\\textbf{P}$, where $T$ is the tag set. In my\n",
    "implementation, $\\textbf{P}_{j,k}$ is the score of transitioning\n",
    "to tag $j$ from tag $k$. So:\n",
    "\n",
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n",
    "\n",
    "\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}\n",
    "\n",
    "where in this second expression, we think of the tags as being assigned\n",
    "unique non-negative indices.\n",
    "\n",
    "If the above discussion was too brief, you can check out\n",
    "`this <http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__ write up from\n",
    "Michael Collins on CRFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Implementation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we will see a full, complicated example of a Bi-LSTM\n",
    "Conditional Random Field for named-entity recognition. The LSTM tagger\n",
    "above is typically sufficient for part-of-speech tagging, but a sequence\n",
    "model like the CRF is really essential for strong performance on NER.\n",
    "Familiarity with CRF's is assumed. Although this name sounds scary, all\n",
    "the model is is a CRF but where an LSTM provides the features. This is\n",
    "an advanced model though, far more complicated than any earlier model in\n",
    "this tutorial. If you want to skip it, that is fine. To see if you're\n",
    "ready, see if you can:\n",
    "\n",
    "-  Write the recurrence for the viterbi variable at step i for tag k.\n",
    "-  Modify the above recurrence to compute the forward variables instead.\n",
    "-  Modify again the above recurrence to compute the forward variables in\n",
    "   log-space (hint: log-sum-exp)\n",
    "\n",
    "If you can do those three things, you should be able to understand the\n",
    "code below. Recall that the CRF computes a conditional probability. Let\n",
    "$y$ be a tag sequence and $x$ an input sequence of words.\n",
    "Then we compute\n",
    "\n",
    "\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}\n",
    "\n",
    "Where the score is determined by defining some log potentials\n",
    "$\\log \\psi_i(x,y)$ such that\n",
    "\n",
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}\n",
    "\n",
    "To make the partition function tractable, the potentials must look only\n",
    "at local features.\n",
    "\n",
    "In the Bi-LSTM CRF, we define two kinds of potentials: emission and\n",
    "transition. The emission potential for the word at index $i$ comes\n",
    "from the hidden state of the Bi-LSTM at timestep $i$. The\n",
    "transition scores are stored in a $|T|x|T|$ matrix\n",
    "$\\textbf{P}$, where $T$ is the tag set. In my\n",
    "implementation, $\\textbf{P}_{j,k}$ is the score of transitioning\n",
    "to tag $j$ from tag $k$. So:\n",
    "\n",
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n",
    "\n",
    "\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}\n",
    "\n",
    "where in this second expression, we think of the tags as being assigned\n",
    "unique non-negative indices.\n",
    "\n",
    "If the above discussion was too brief, you can check out\n",
    "`this <http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__ write up from\n",
    "Michael Collins on CRFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
